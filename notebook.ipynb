{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a36a97",
   "metadata": {},
   "source": [
    "# A.) Data Visualization and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab4f83",
   "metadata": {},
   "source": [
    "# Introduction to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260e279",
   "metadata": {},
   "source": [
    "It contains the performance of students from two high schools, gathered from a multiple-choice questionnaire.\n",
    "\n",
    "1. school - student's school (GP or MS)\n",
    "2. sex - student's gender (female or male)\n",
    "3. age - student's age (15-22)\n",
    "4. address - student's residence (rural or urban)\n",
    "5. famsize - family size (greater than 3 or less than 3)\n",
    "6. Pstatus - parents' cohabitation status (together or apart)\n",
    "7. Medu - mother's highest education level (0 - none, 1 - primary (4th grade), 2 - 5th-9th grade, 3 - high school, 4 - higher education)\n",
    "8. Fedu - father's highest education level (0 - none, 1 - primary (4th grade), 2 - 5th-9th grade, 3 - high school, 4 - higher education)\n",
    "9. Mjob - mother's job (teacher, health, public service, at home, other)\n",
    "10. Fjob - father's job (teacher, health, public service, at home, other)\n",
    "11. reason - reason for choosing the school (close to home, school reputation, course, other)\n",
    "12. guardian - student's guardian (mother, father, other)\n",
    "13. traveltime - travel time (1 - <15 minutes, 2 - 15 to 30 minutes, 3 - 0.5-1 hour, 4 - >1 hour)\n",
    "14. studytime - weekly study time (1 - <2 hours, 2 - 2-5 hours, 3 - 5-10 hours, 4 - >10 hours)\n",
    "15. failures - number of previous failures (n - 1<=n<3, otherwise 4)\n",
    "16. schoolsup - extra educational support (yes or no)\n",
    "17. famsup - family support (yes or no)\n",
    "18. paid - paid extra classes in course subjects (yes or no)\n",
    "19. activities - extracurricular activities (yes or no)\n",
    "20. nursery - attended nursery school (yes or no)\n",
    "21. higher - wants to attend higher education (yes or no)\n",
    "22. internet - has internet at home (yes or no)\n",
    "23. romantic - in a relationship (yes or no)\n",
    "24. famrel - quality of family relationships (1 - very bad, 5 - excellent)\n",
    "25. freetime - free time (1 - very little, 5 - very high)\n",
    "26. goout - going out with friends (1 - very little, 5 - very high)\n",
    "27. 28. Dalc, Walc - weekday and weekend alcohol consumption (1 - very little, 5 - very high)\n",
    "29. health - current health status (1 - very bad, 5 - very good)\n",
    "30. absences - school absences (0-93)\n",
    "\n",
    "31. G1 - first semester grade (0-20)\n",
    "32. G2 - second semester grade (0-20)\n",
    "33. G3 - final grade (0-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the grades ('fail' - 0-9, 'sufficient' - 10-11, 'satisfactory' - 12-13, 'good' - 14-15, 'excellent' - 16-20)\n",
    "\n",
    "def create_average():\n",
    "    columns = ['G1', 'G2', 'G3']\n",
    "    df['annual_grades_avg'] = df[columns].mean(axis=1)\n",
    "\n",
    "create_average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_level_classification():\n",
    "    bins = pd.IntervalIndex.from_tuples(\n",
    "        [(0, 9.5), (9.5, 11.5), (11.5, 13.5), (13.5, 15.5), (15.5, 20)], closed='right')\n",
    "\n",
    "    levels = ['fail', 'sufficient', 'satisfactory', 'good', 'excellent']\n",
    "\n",
    "    new_column = 'annual_grades_evaluation'\n",
    "    df[new_column] = np.array(levels)[pd.cut(df['annual_grades_avg'], bins=bins).cat.codes]\n",
    "\n",
    "five_level_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 395 records, 33 attributes\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381de804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6de72",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_insight(categorical_columns):\n",
    "\n",
    "    nrows, ncols = categorical_columns.shape[1], 3\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(25, 85))\n",
    "\n",
    "    for idx, column in enumerate(categorical_columns):\n",
    "        ax = axes[idx]\n",
    "        sns.countplot(data=df, x='annual_grades_evaluation', hue=column, ax=ax[0])\n",
    "\n",
    "        sns.countplot(data=df, x=column, ax=ax[1])\n",
    "\n",
    "        sns.boxplot(data=df, x=column, y='annual_grades_avg', ax=ax[2])\n",
    "        \n",
    "columns = df.select_dtypes(include='object')\n",
    "columns = columns.drop('annual_grades_evaluation', axis=1)\n",
    "plot_categorical_insight(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38e67a",
   "metadata": {},
   "source": [
    "- One school outperforms the other (even in terms of failures)\n",
    "- Girls fail more often than boys\n",
    "- Rural students perform better\n",
    "- Those engaged in extracurricular activities perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02522f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['fail', 'sufficient', 'satisfactory', 'good', 'excellent']\n",
    "\n",
    "def plot_grades_to_self():\n",
    "    nrows, ncols = 1, 2\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 8))\n",
    "    sns.countplot(data=df, x='annual_grades_evaluation', ax=axes[0], order=levels)\n",
    "\n",
    "    sns.boxplot(data=df, x='annual_grades_evaluation', y='annual_grades_avg', ax=axes[1], order=levels)\n",
    "\n",
    "plot_grades_to_self()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391c701",
   "metadata": {},
   "source": [
    "Clearly, many students have failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51722605",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.corr()\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(corr, annot=True, cmap='crest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4056f",
   "metadata": {},
   "source": [
    "- All three grades are highly positively correlated with each other.\n",
    "- Parents' highest education levels are moderately positively correlated.\n",
    "- Weekend and weekday alcohol consumption are positively correlated. Those who consume a lot on weekends do so on weekdays too.\n",
    "- Failures, unsurprisingly, are negatively correlated with grades; why would better grades follow more failures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e09631",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df[['annual_grades_avg','absences', 'studytime', 'failures']]\n",
    "sns.pairplot(data=dfd);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c836a90",
   "metadata": {},
   "source": [
    "Regarding clustering, the plot of 'absences' and 'annual_grades_avg' is more interesting to me, so I will continue with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb001bed",
   "metadata": {},
   "source": [
    "# Preprocessing, Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d6f13",
   "metadata": {},
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a991f",
   "metadata": {},
   "source": [
    "From this point, we will examine the averaged grade column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop = ['G1', 'G2', 'G3']\n",
    "df = df.drop(column_to_drop,  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548698a9",
   "metadata": {},
   "source": [
    "Removing extreme values based on the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74502a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(columns):\n",
    "    outlier_indices = []\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 =df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1  # interkvartilis tábolság\n",
    "\n",
    "        mask = (df[column] >= Q1 - 1.5 *\n",
    "                IQR) & (df[column] <= Q3 + 1.5 * IQR)\n",
    "        mask = mask.to_numpy()\n",
    "        false_indices = np.argwhere(~mask)\n",
    "        outlier_indices.append(false_indices)\n",
    "    return np.unique(np.concatenate(outlier_indices).ravel())\n",
    "\n",
    "numerical_columns = ['age', 'absences', 'annual_grades_avg']\n",
    "outlier_indices = detect_outliers(numerical_columns)\n",
    "\n",
    "print(f'Number of outliers: {len(outlier_indices)}')\n",
    "\n",
    "df = df.drop(outlier_indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa3c6f",
   "metadata": {},
   "source": [
    "Converting all variables to float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in df:\n",
    "    df[i] = le.fit_transform(df[i]).astype(float)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573212bf",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "K-means, or k-means clustering, is a centroid-based clustering algorithm (data points cluster around a centroid). During clustering, it only matters which centroid is closest to each point based on Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da568ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df[['absences','annual_grades_avg']])\n",
    "\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22be478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_result(kmeans):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=df['absences'], y=df['annual_grades_avg'], hue=kmeans.labels_, palette=\"Set1\", marker='+');\n",
    "    sns.scatterplot(x=kmeans.cluster_centers_[:,0], y=kmeans.cluster_centers_[:,1], marker='o', s=100, c=['black'])\n",
    "    \n",
    "plot_result(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8c624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err = []\n",
    "for i in range(1,20):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(df[['absences','annual_grades_avg']]);\n",
    "    err.append([kmeans.n_clusters, kmeans.inertia_])\n",
    "\n",
    "err = np.asarray(err)\n",
    "sns.lineplot(x=err[:,0], y=err[:,1])\n",
    "sns.scatterplot(x=err[:,0], y=err[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537de92a",
   "metadata": {},
   "source": [
    "The plot suggests that 5 clusters seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86565a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps = 5, min_samples = 10).fit(df)\n",
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_clusters_\n",
    "\n",
    "sns.scatterplot(data = df, x = 'absences', y = 'annual_grades_avg', hue = db.labels_, legend = 'full', palette = 'deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099537a",
   "metadata": {},
   "source": [
    "# Regression and Classification Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51821bd",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and test sets in a 70% training and 30% test ratio, as the training set would not be large enough to make the estimated accuracy unreliable otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea0e50",
   "metadata": {},
   "source": [
    "Normalizing the data using MinMaxScaler. Each feature is scaled individually and transformed to be within the specified range on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedda0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "test = pd.DataFrame(scaler.fit_transform(test), columns = test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe56d95",
   "metadata": {},
   "source": [
    "Function for building the classification model and achieving performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d906464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold as KFold   # K-fold keresztérvényesítéshez\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def classification_model(model, data, predictors, outcome, param_grid):\n",
    "\n",
    "# Modellillesztés:\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "  # 'Jóslás' a tanulóhalmazon:\n",
    "  predictions = model.predict(data[predictors])\n",
    "\n",
    "  # Pontosság kiíratása\n",
    "  accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "  print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "    \n",
    "  # K-fold keresztérvényesítás 4 'folddal'\n",
    "  kf = KFold(n_splits=4, n_repeats=2)\n",
    "  error = []\n",
    "  for train, test in kf.split(data[predictors], data[outcome]):\n",
    "    # Tanulóhalmaz szűrése\n",
    "    train_predictors = (data[predictors].iloc[train,:])\n",
    "    \n",
    "    # A célpont, amit az algoritmus betanításához használunk.\n",
    "    train_target = data[outcome].iloc[train]\n",
    "    \n",
    "    # Az algoritmus betanítása a prediktorok és a célpont segítségével\n",
    "    model.fit(train_predictors, train_target)\n",
    "    \n",
    "    # Hiba rögzítése minden keresztellenőrzési futtatásból\n",
    "    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    "\n",
    "  print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n",
    "\n",
    "  # Modellillesztés, hogy a függvényen kívül is hivatkozhassunk rá\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "\n",
    "  # Hiperparaméterek finomhangolása\n",
    "  grid = GridSearchCV(estimator=model,param_grid=param_grid,scoring=\"accuracy\",cv=4)\n",
    "\n",
    "  grid.fit(data[predictors],data[outcome])\n",
    "  print(f\"BEST SCORE: %s\" % \"{0:.3%}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "outcome_var = 'higher'\n",
    "predictor_var = ['address']\n",
    "param_grid = [{'penalty':['none','l2']}, \n",
    "              {'C':[1, 10, 100, 1000]}]\n",
    "classification_model(model, train, predictor_var, outcome_var, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5207483",
   "metadata": {},
   "source": [
    "Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c263173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44a520",
   "metadata": {},
   "source": [
    "The coefficient of the features in the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e78f9",
   "metadata": {},
   "source": [
    "The format of the trained model here is a decision tree. A decision tree is a tree where each internal node represents a feature. The children of a node represent the possible values of that feature. The leaves contain class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "predictor_var = ['school', 'sex', 'address', 'activities']\n",
    "param_grid = [{'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}]\n",
    "classification_model(model, train ,predictor_var, outcome_var, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028ba85",
   "metadata": {},
   "source": [
    "The essence of so-called random forests is that it creates several different decision trees, unlike the simple decision tree method, and provides the final result by averaging their outcomes. Since decision trees are very sensitive to changes in the training dataset, it replaces random elements in the dataset with other repeated elements. Different features are used for training during the creation of each decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09feb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "predictor_var = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery']\n",
    "param_grid = [{'n_estimators': [5,20,50,100],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4]}]\n",
    "classification_model(model, train, predictor_var, outcome_var, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c67fdc",
   "metadata": {},
   "source": [
    "In general, it can be stated that for this dataset, the random forest provides better accuracy than the decision tree and logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
